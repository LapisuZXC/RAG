{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AlexxFlexing/RAG/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    #model_name = \"ai-forever/sbert_large_mt_nlu_ru\",\n",
    "    model_name = \"sergeyzh/rubert-mini-uncased\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True} # i wanted to explicit set this to False since we are using weaviate, but DONT FORGET TO SET IT TO TRUE if we stop using weaviate. SET IT TO TRUE if vector db doesnt normalize automatically\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/AlexxFlexing/RAG/venv/lib/python3.12/site-packages/weaviate/warnings.py:340: UserWarning: Con006: You're using the sync client in an async context. This usage is discouraged to avoid blocking your async event loop with sync I/O calls.\n",
      "            We encourage you to update your code to use the async client instead when running inside async def functions!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x73e94f586c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader #not sure about those 2\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader #i mean this as second\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "#series_result = '../data/processed/series_results.csv'\n",
    "#df = pd.read_csv(series_result)\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    host=\"127.0.0.1\",  # Use a string to specify the host\n",
    "    port=8080,\n",
    "    grpc_port=50051,\n",
    ")\n",
    "\n",
    "if client.collections.exists(\"MatchData\"): #redo this in prod obviously\n",
    "    client.collections.delete(\"MatchData\")\n",
    "\n",
    "if client.collections.exists(\"MapStats\"):\n",
    "    client.collections.delete(\"MapStats\")\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"MatchData\",\n",
    "    properties=[\n",
    "        Property(name=\"team_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"team_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"date\", data_type=DataType.TEXT),\n",
    "        Property(name=\"opponent_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"opponent_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"map_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"series_result\", data_type=DataType.TEXT),\n",
    "        Property(name=\"description\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    description=\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–∞—Ç—á–µ –º–µ–∂–¥—É –∫–æ–º–∞–Ω–¥–∞–º–∏\",\n",
    "    vectorizer_config=Configure.Vectorizer.none()  # for manual embedding\n",
    ")\n",
    "\n",
    "# Create MapStats class\n",
    "client.collections.create(\n",
    "    name=\"MapStats\",\n",
    "    properties=[\n",
    "        Property(name=\"team_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"map_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"winrate\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"pickrate\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"banrate\", data_type=DataType.NUMBER),\n",
    "    ],\n",
    "    description=\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ä—Ç–∞–º –¥–ª—è –∫–æ–º–∞–Ω–¥\",\n",
    "    vectorizer_config=Configure.Vectorizer.none()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456/456 [00:02<00:00, 169.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"sergeyzh/rubert-mini-uncased\")\n",
    "\n",
    "series_result = '../data/processed/series_results.csv'\n",
    "maps_stats = '../data/processed/team_maps.csv'\n",
    "\n",
    "matches = pd.read_csv(series_result)\n",
    "matches.drop(columns=['series_link', 'series_result'])\n",
    "maps = pd.read_csv(maps_stats)\n",
    "\n",
    "\n",
    "\n",
    "matches[\"description\"] = matches.apply(\n",
    "    lambda row: f\"–∫–æ–º–∞–Ω–¥–∞ {row['team_name']} —Å—ã–≥—Ä–∞–ª–∞ –ø—Ä–æ—Ç–∏–≤ –∫–æ–º–∞–Ω–¥—ã {row['opponent_name']} –Ω–∞ –∫–∞—Ä—Ç–µ {row['map_name']}, –º–∞—Ç—á –±—ã–ª –ø—Ä–æ–≤–µ–¥–µ–Ω {row['date']}\",\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = model.encode(matches[\"description\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "collection_match = client.collections.get(\"MatchData\") #fix collections later maybe\n",
    "\n",
    "def json_clean(obj):\n",
    "    for k, v in obj.items():\n",
    "        if isinstance(v, float) and (np.isnan(v) or np.isinf(v)):\n",
    "            obj[k] = 0.0\n",
    "    return obj\n",
    "\n",
    "\n",
    "for i, row in matches.iterrows():\n",
    "    properties = {\n",
    "        \"team_id\": str(row[\"team_id\"]),\n",
    "        \"team_name\": row[\"team_name\"],\n",
    "        \"date\": row[\"date\"],\n",
    "        \"opponent_id\": str(row[\"opponent_id\"]),\n",
    "        \"opponent_name\": row[\"opponent_name\"],\n",
    "        \"map_name\": row[\"map_name\"],\n",
    "        \"series_result\": str(row[\"series_result\"]),\n",
    "        \"description\": row[\"description\"],\n",
    "    }\n",
    "    properties = json_clean(properties) # had to\n",
    "    vector = embeddings[i]\n",
    "    collection_match.data.insert(properties=properties, vector=vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "collection_team_map = client.collections.get(\"MapStats\")\n",
    "df = pd.read_csv(maps_stats)\n",
    "\n",
    "def extract_float(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    match = re.search(r\"[\\d.]+\", str(value))\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "for col in ['winrate', 'pickrate', 'banrate']:\n",
    "    df[col] = df[col].apply(extract_float)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    properties = {\n",
    "        \"team_id\": str(row[\"team_id\"]),\n",
    "        \"map_name\": row[\"map_name\"],\n",
    "        \"winrate\": float(row[\"winrate\"]),\n",
    "        \"pickrate\": float(row[\"pickrate\"]),\n",
    "        \"banrate\": float(row[\"banrate\"]),\n",
    "    }\n",
    "    collection_team_map.data.insert(properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "match_vectorstore = WeaviateVectorStore(\n",
    "    client=client,\n",
    "    index_name=\"MatchData\",  # –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞\n",
    "    text_key=\"description\",  # –ø–æ –∫–∞–∫–æ–º—É –ø–æ–ª—é –∏—Å–∫–∞—Ç—å\n",
    "    embedding=embedding_model,    # —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    ")\n",
    "\n",
    "match_retriever_certain = match_vectorstore.as_retriever(search_kwargs={\"k\":1}) # –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –æ–¥–∏–Ω, –∞ –¥–∞–∂–µ 5, –µ—Å–ª–∏ bo5. –¥—É–º–∞—Ç—å.\n",
    "match_retriever_bo3 = match_vectorstore.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_instructions_ru = \"\"\"–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ –æ–¥–Ω–æ–º—É –∏–∑ –∞–≥–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "–ê–≥–µ–Ω—Ç—ã:\n",
    "- –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∞—Ç—á–∞ ‚Üí \"predictor\"\n",
    "- –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ñ–æ—Ä–º—ã –∏–≥—Ä–æ–∫–∞ –∏–ª–∏ –∫–æ–º–∞–Ω–¥—ã ‚Üí \"shape\"\n",
    "- –∞–Ω–∞–ª–∏–∑ –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã ‚Üí \"team\"\n",
    "- —Å–≤–æ–±–æ–¥–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ ‚Üí \"fallback\"\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç–∞ –≤–µ—Ä–Ω–∏ **–¢–û–õ–¨–ö–û** JSON-–æ–±—ä–µ–∫—Ç —Å –∫–ª—é—á–æ–º \"node\", –æ–¥–Ω–æ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏–π: \"predictor\", \"shape\", \"team\", \"fallback\".\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "{\"node\": \"predictor\"}\n",
    "\n",
    "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π. –ù–µ –≤–µ–¥–∏ –¥–∏–∞–ª–æ–≥. –¢–æ–ª—å–∫–æ JSON.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# –≤–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏—Ç –¥–æ–±–∞–≤–∏—Ç—å bias –≤ —Å—Ç–æ—Ä–æ–Ω—É fallback, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å —á–∞—â–µ –≤—ã–¥–∞–≤–∞–ª–∞ –µ–≥–æ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"ilyagusev/saiga_llama3:latest\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 'predictor'} {'node': 'fallback'} {'node': 'shape'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "\n",
    "test_case_match = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions_ru)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"–∫—Ç–æ –≤—ã–∏–≥—Ä–∞–µ—Ç –≤ –∑–∞–≤—Ç—Ä–∞—à–Ω–µ–º –º–∞—Ç—á–µ: Navi –∏–ª–∏ G2?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "test_case_fallback = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions_ru)]\n",
    "    + [HumanMessage(content=\"–Ø –≤–∫—É—Å–Ω–æ –ø–æ–∫—É—à–∞–ª\")]\n",
    ")\n",
    "test_case_shape = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions_ru)]\n",
    "    + [HumanMessage(content=\"–í –∫–∞–∫–æ–π —Ñ–æ—Ä–º–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è Niko?\")]\n",
    ")\n",
    "print(\n",
    "    json.loads(test_case_match.content),\n",
    "    json.loads(test_case_fallback.content),\n",
    "    json.loads(test_case_shape.content),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Close win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643280/374847762.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Prediction:\", labels[int(pred[0])])\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "closeness_analyzer_model = CatBoostClassifier()\n",
    "closeness_analyzer_model.load_model(\"catboost_series_model.cbm\")\n",
    "\n",
    "sample_feature = pd.DataFrame([[55.0, 10.0, 20.0, 50.0, 8.0, 30.0]], columns=[\n",
    "    'team_map_winrate', 'team_map_pickrate', 'team_map_banrate',\n",
    "    'opponent_map_winrate', 'opponent_map_pickrate', 'opponent_map_banrate'\n",
    "])\n",
    "\n",
    "# Predict\n",
    "pred = closeness_analyzer_model.predict(sample_feature)\n",
    "\n",
    "labels = [\"One-sided win\", \"Close win\", \"Close loss\", \"One-sided loss\"]\n",
    "print(\"Prediction:\", labels[int(pred[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team_name': 'Natus Vincere', 'opponent_name': 'G2 Esports', 'date': None, 'map_name': 'Mirage'} {'team_name': 'Team Spirit', 'opponent_name': 'Virtus Pro', 'date': '25/10/2023', 'map_name': None} {'team_name': None, 'opponent_name': None, 'date': 'null', 'map_name': None}\n",
      "{'team_name': 'null', 'opponent_name': 'null'}\n"
     ]
    }
   ],
   "source": [
    "# match predictor section\n",
    "test_case_prompt = \"\"\"–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –µ—Å–ª–∏ –æ–Ω–∏ —É–∫–∞–∑–∞–Ω—ã. –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã –Ω–µ—Ç, —Ç–æ —É–∫–∞–∂–∏ 'null'\n",
    "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–æ–∑–≤—Ä–∞—â–µ–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∫–ª—é—á–∞–º–∏:\n",
    "{\n",
    "    \"team_name\": \"<–Ω–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–π –∫–æ–º–∞–Ω–¥—ã –∏–ª–∏ null>\",\n",
    "    \"opponent_name\": \"<–Ω–∞–∑–≤–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–π –∫–æ–º–∞–Ω–¥—ã –∏–ª–∏ null>\"    \n",
    "}\n",
    "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–π —á–∏—Å—Ç—ã–π JSON-–æ–±—ä–µ–∫—Ç.\n",
    "\"\"\"\n",
    "match_predictor_prompt = \"\"\"–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∏–∑–≤–ª–µ—á—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–ª–µ–¥—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ:\n",
    "–í —Ç–≤–æ–µ–π –ø–∞–º—è—Ç–∏ –Ω–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –∏–≥—Ä.\n",
    "üîπ –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n",
    "- –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–π –∫–æ–º–∞–Ω–¥—ã (team_name)\n",
    "- –ù–∞–∑–≤–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–π –∫–æ–º–∞–Ω–¥—ã (opponent_name)\n",
    "\n",
    "üîπ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã):\n",
    "- –î–∞—Ç–∞ –º–∞—Ç—á–∞ (date) ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ dd/mm/yyyy (–Ω–∞–ø—Ä–∏–º–µ—Ä: 25/11/2024)\n",
    "- –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã (map_name)\n",
    "\n",
    "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–æ–∑–≤—Ä–∞—â—ë–Ω —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∫–ª—é—á–∞–º–∏:\n",
    "\n",
    "{\n",
    "  \"team_name\": \"<–Ω–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–π –∫–æ–º–∞–Ω–¥—ã –∏–ª–∏ null>\",\n",
    "  \"opponent_name\": \"<–Ω–∞–∑–≤–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–π –∫–æ–º–∞–Ω–¥—ã –∏–ª–∏ null>\",\n",
    "  \"date\": \"<–¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ dd/mm/yyyy –∏–ª–∏ null>\",\n",
    "  \"map_name\": \"<–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã –∏–ª–∏ null>\"\n",
    "}\n",
    "\n",
    "–ï—Å–ª–∏ –∫–∞–∫–∏–µ-–ª–∏–±–æ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –∑–∞–ø—Ä–æ—Å–µ, —É–∫–∞–∂–∏ –∑–Ω–∞—á–µ–Ω–∏–µ `null` (—ç—Ç–æ JSON null, –±–µ–∑ –∫–∞–≤—ã—á–µ–∫). –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON ‚Äî —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π JSON-–æ–±—ä–µ–∫—Ç.\n",
    "–ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã –Ω–µ—Ç, —Ç–æ —É–∫–∞–∂–∏ 'null'\n",
    "–ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã –Ω–µ—Ç, —Ç–æ —É–∫–∞–∂–∏ 'null'\n",
    "–ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã –Ω–µ—Ç, —Ç–æ —É–∫–∞–∂–∏ 'null'\n",
    "–ü—Ä–∏–º–µ—Ä –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞:\n",
    "\n",
    "{\n",
    "  \"team_name\": \"Team Spirit\",\n",
    "  \"opponent_name\": \"Natus Vincere\",\n",
    "  \"date\": \"25/11/2024\",\n",
    "  \"map_name\": \"Mirage\"\n",
    "}\n",
    "\n",
    "–ë—É–¥—å –≤–Ω–∏–º–∞—Ç–µ–ª–µ–Ω –∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º ‚Äî –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥, –¥–∞—Ç—É –∏ –∫–∞—Ä—Ç—É –º–æ–≥—É—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –≤ —Å–≤–æ–±–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–µ. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ç–æ—á–Ω–æ –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Ä–Ω—É—Ç—å –∏—Ö –≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.\n",
    "\"\"\"\n",
    "\n",
    "test_case_match_pred_1 = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"–Ω–∞—Å–∫–æ–ª—å–∫–æ –±–ª–∏–∑–∫–æ–π –±—ã–ª–∞ –∏–≥—Ä–∞ Navi - G2 –Ω–∞ –∫–∞—Ä—Ç–µ –º–∏—Ä–∞–∂\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "test_case_match_pred_2 = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [HumanMessage(content=\"–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–≥—Ä—É Team Spirit –ø—Ä–æ—Ç–∏–≤ Virtus Pro 25 –æ–∫—Ç—è–±—Ä—è 2023 –≥–æ–¥–∞\")]\n",
    ")\n",
    "test_case_match_pred_3 = llm_json_mode.invoke(       # fix this ffs\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [HumanMessage(content=\"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—á–µ—Ä–∞—à–Ω—é—é –∏–≥—Ä—É\")]\n",
    ")\n",
    "print(\n",
    "    json.loads(test_case_match_pred_1.content),\n",
    "    json.loads(test_case_match_pred_2.content),\n",
    "    json.loads(test_case_match_pred_3.content),\n",
    ")\n",
    "\n",
    "hard_test_case = llm_json_mode.invoke([SystemMessage(content=test_case_prompt)]\n",
    "    + [HumanMessage(content=\"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—á–µ—Ä–∞—à–Ω—é—é –∏–≥—Ä—É\")]\n",
    ")\n",
    "\n",
    "print(json.loads(hard_test_case.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    initial_prompt: str # initial user prompt\n",
    "    generation: str # LLM generation\n",
    "    max_retries: int # max number of retries for answering\n",
    "    answers: int # number of answers generated\n",
    "    loop_step: Annotated[int, operator.add] # have to use annotated int since using default int will lead into multiple edges not being able to combine values properly (and some other stuff)\n",
    "    source: List[str] # stats or any other retrieved valuables\n",
    "    #source: dict\n",
    "    #extra_source: dict\n",
    "    extra_source: List[str] #i ve made this source just to be sure it is not overridden, del later or fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Close win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643280/85189475.py:50: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Prediction:\", labels[int(pred[0])])\n"
     ]
    }
   ],
   "source": [
    "match_doc = match_retriever_certain.invoke('Heroic –ø—Ä–æ—Ç–∏–≤ G2')\n",
    "def match_metadata_fetch(match_doc):\n",
    "    matchdoc_data = match_doc[0].metadata\n",
    "    team_id = str(re.sub(r'\\.\\d*', '', matchdoc_data['team_id']))\n",
    "    opponent_id = re.sub(r'\\.\\d*', '', matchdoc_data['opponent_id'])\n",
    "    map_name = matchdoc_data['map_name'] #dont ask why those str turned into float\n",
    "    return team_id, opponent_id, map_name\n",
    "\n",
    "team_id, opponent_id, map_name = match_metadata_fetch(match_doc)\n",
    "\n",
    "\n",
    "def team_map_stats(team_id, opponent_id, map_name):\n",
    "    response = collection_team_map.query.fetch_objects(\n",
    "        filters=(Filter.by_property(\"team_id\").equal(team_id)) & Filter.by_property(\"map_name\").equal(map_name)\n",
    "    )\n",
    "    if response.objects:\n",
    "        team1_winrate = response.objects[0].properties['winrate']\n",
    "        team1_pickrate = response.objects[0].properties['pickrate']\n",
    "        team1_banrate = response.objects[0].properties['banrate']\n",
    "\n",
    "    response = collection_team_map.query.fetch_objects(\n",
    "        filters=(Filter.by_property(\"team_id\").equal(opponent_id)) & Filter.by_property(\"map_name\").equal(map_name)\n",
    "    )\n",
    "    if response.objects:\n",
    "        team2_winrate = response.objects[0].properties['winrate']\n",
    "        team2_pickrate = response.objects[0].properties['pickrate']\n",
    "        team2_banrate = response.objects[0].properties['banrate']\n",
    "    df = pd.DataFrame([[\n",
    "        team1_winrate,\n",
    "        team1_pickrate,\n",
    "        team1_banrate,\n",
    "        team2_winrate,\n",
    "        team2_pickrate,\n",
    "        team2_banrate\n",
    "    ]], columns=[\n",
    "        'team_map_winrate',\n",
    "        'team_map_pickrate',\n",
    "        'team_map_banrate',\n",
    "        'opponent_map_winrate',\n",
    "        'opponent_map_pickrate',\n",
    "        'opponent_map_banrate'\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "sample_map_stats = team_map_stats(team_id, opponent_id, map_name)\n",
    "\n",
    "pred = closeness_analyzer_model.predict(sample_map_stats)\n",
    "labels = [\"One-sided win\", \"Close win\", \"Close loss\", \"One-sided loss\"]\n",
    "print(\"Prediction:\", labels[int(pred[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_source = {\\n    \"date\":\\'null\\',\\n    \"map_name\":\\'Mirage\\'\\n}\\n\\nsample_query = \\'Heroic –ø—Ä–æ—Ç–∏–≤ G2 –Ω–∞ Mirage\\'\\n\\nfilter_sample = []\\nif sample_source[\\'date\\'] != \\'null\\' or sample_source[\\'date\\'] != \\'None\\':\\n    filter_sample.append(Filter.by_property(\"date\").equal(sample_source[\"date\"]))\\nif sample_source[\\'map_name\\'] != \\'null\\' or sample_source[\\'map_name\\'] != \\'None\\':\\n    filter_sample.append(Filter.by_property(\"map_name\").equal(sample_source[\"map_name\"]))\\n\\ncombined_filter = None\\nif filter_sample:\\n    combined_filter = Filter.all_of([*filter_sample])\\nresponse = collection_match.query.near_text(\\n    query=sample_query,\\n    filters=combined_filter,\\n    limit=3\\n)\\n\\nprint(response)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is query + filter\n",
    "# it is not working\n",
    "# why?\n",
    "# because i fucked up big time\n",
    "# how?\n",
    "# by tweaking manual embedding\n",
    "\n",
    "\"\"\"sample_source = {\n",
    "    \"date\":'null',\n",
    "    \"map_name\":'Mirage'\n",
    "}\n",
    "\n",
    "sample_query = 'Heroic –ø—Ä–æ—Ç–∏–≤ G2 –Ω–∞ Mirage'\n",
    "\n",
    "filter_sample = []\n",
    "if sample_source['date'] != 'null' or sample_source['date'] != 'None':\n",
    "    filter_sample.append(Filter.by_property(\"date\").equal(sample_source[\"date\"]))\n",
    "if sample_source['map_name'] != 'null' or sample_source['map_name'] != 'None':\n",
    "    filter_sample.append(Filter.by_property(\"map_name\").equal(sample_source[\"map_name\"]))\n",
    "\n",
    "combined_filter = None\n",
    "if filter_sample:\n",
    "    combined_filter = Filter.all_of([*filter_sample])\n",
    "response = collection_match.query.near_text(\n",
    "    query=sample_query,\n",
    "    filters=combined_filter,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "print(response)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# –æ—á–µ–Ω—å –≥–ª—É–ø—ã–π –∏ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç \n",
    "predictor_final_prompt = \"\"\"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –º–∞—Ç—á–µ–π –≤ –∫–∏–±–µ—Ä—Å–ø–æ—Ä—Ç–µ, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –Ω–∞–¥ –∑–∞–¥–∞—á–∞–º–∏ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –≤ —Ä–∞–º–∫–∞—Ö –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.\n",
    "\n",
    "–í–æ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞:\n",
    "\n",
    "{context}\n",
    "\n",
    "–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–π –º–∞—Ç—á:\n",
    "\n",
    "{user_prompt}\n",
    "\n",
    "–¢–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –º–∞—Ç—á—É:\n",
    "\n",
    "{extra_source}\n",
    "\n",
    "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —á–∏—Å–ª–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ä—Ç–µ –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø–æ—Ä—è–¥–∫–µ:\n",
    "- team_map_winrate ‚Äî –≤–∏–Ω—Ä–µ–π—Ç –∫–æ–º–∞–Ω–¥—ã –Ω–∞ –∫–∞—Ä—Ç–µ\n",
    "- team_map_pickrate ‚Äî —á–∞—Å—Ç–æ—Ç–∞ –ø–∏–∫–∞ –∫–∞—Ä—Ç—ã –∫–æ–º–∞–Ω–¥–æ–π\n",
    "- team_map_banrate ‚Äî —á–∞—Å—Ç–æ—Ç–∞ –±–∞–Ω–∞ –∫–∞—Ä—Ç—ã –∫–æ–º–∞–Ω–¥–æ–π\n",
    "- opponent_map_winrate ‚Äî –≤–∏–Ω—Ä–µ–π—Ç —Å–æ–ø–µ—Ä–Ω–∏–∫–∞ –Ω–∞ –∫–∞—Ä—Ç–µ\n",
    "- opponent_map_pickrate ‚Äî —á–∞—Å—Ç–æ—Ç–∞ –ø–∏–∫–∞ –∫–∞—Ä—Ç—ã —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–º\n",
    "- opponent_map_banrate ‚Äî —á–∞—Å—Ç–æ—Ç–∞ –±–∞–Ω–∞ –∫–∞—Ä—Ç—ã —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–º\n",
    "\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞:\n",
    "1. –ü—Ä–∏–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.\n",
    "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞, —á—Ç–æ–±—ã –æ–±—ä—è—Å–Ω–∏—Ç—å, –ø–æ—á–µ–º—É –º–∞—Ç—á —Å–ª–æ–∂–∏–ª—Å—è –∏–º–µ–Ω–Ω–æ —Ç–∞–∫.\n",
    "3. –£—á–∏—Ç—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è.\n",
    "4. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî –æ–ø–∏—Ä–∞–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –≤—ã—à–µ.\n",
    "\n",
    "–î–∞–π —á—ë—Ç–∫–∏–π –∏ –ª–æ–≥–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ú–∞–∫—Å–∏–º—É–º 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.\n",
    "\n",
    "–û—Ç–≤–µ—Ç:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document #for docs but remove before pushing\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# nodes\n",
    "def retrieve_for_match(state):\n",
    "    # –Ω–∞–¥–æ –Ω–∞–π—Ç–∏ –º–∞—Ç—á, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ–∏–ª—å—Ç—Ä—ã\n",
    "    # –Ω–æ —Ñ–∏–ª—å—Ç—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å, –ø–æ—Ç–æ–º—É —á—Ç–æ —è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –º–∞–Ω—É–∞–ª —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ä–∞–Ω–µ–µ (—Ö–∞–∞—Ö–∞—Ö–∞—Ö–∞—Ö–∞—Ö–∞—Ö)\n",
    "    # –∏ —Ç–µ–ø–µ—Ä—å —É –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º–∞, –≤ –æ–¥–Ω–æ–π –∏–∑ –∑–∞–∫–æ–º–µ–Ω—á–µ–Ω–Ω—ã—Ö cells –≤—ã—à–µ –æ –Ω–µ–π –Ω–∞–ø–∏—Å–∞–Ω–æ\n",
    "    # –ø–æ—ç—Ç–æ–º—É –º—ã —Ç—É–ø–æ –Ω–∞—Ö–æ–¥–∏–º –ø–æ –≤–µ–∫—Ç–æ—Ä—É –¥–µ—Å–∫—Ä–∏–ø—à–Ω–∞\n",
    "    # –∑–∞–±–∏–≤–∞—è, –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–∞ —Ñ–∏–ª—å—Ç—Ä—ã\n",
    "    source_raw = match_retriever_certain.invoke(state['initial_prompt'])\n",
    "    # –Ω–∞—à–ª–∏ –º–∞—Ç—á –∏ –¥–∞–ª–µ–µ —Ä–∞–±–æ—Ç–∞–µ–º —Å —ç—Ç–∏–º\n",
    "    matchdoc_data = source_raw[0].metadata # –∏ —Ç—É—Ç –º—ã –±–µ—Ä–µ–º –≤—Å–µ–≥–æ –ª–∏—à—å –ø–µ—Ä–≤—ã–π –æ–±—ä–µ–∫—Ç, —Ö–æ—Ç—è –º—ã –µ–≥–æ –±–µ—Ä–µ–º –µ—â–µ —Å –ø–æ–º–æ—â—å—é top k, –≥–¥–µ k=1 –∏—Å–ø–æ–ª—å–∑—É—è retriever_certain\n",
    "    team_id = str(re.sub(r'\\.\\d*', '', matchdoc_data['team_id']))\n",
    "    opponent_id = re.sub(r'\\.\\d*', '', matchdoc_data['opponent_id'])\n",
    "    map_name = matchdoc_data['map_name'] #dont ask why those str turned into float\n",
    "    features_map_stats = team_map_stats(team_id, opponent_id, map_name) # –ø–æ —Ö–æ—Ä–æ—à–µ–º—É –Ω—É–∂–Ω–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏—Ç—å –∫–æ–¥, —á—Ç–æ–±—ã —Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±—ã–ª–∞ –≥–¥–µ –Ω–∏–±—É–¥—å –∑–¥–µ—Å—å)\n",
    "    pred = closeness_analyzer_model.predict(features_map_stats)\n",
    "    labels = [\"–û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –ø–æ–±–µ–¥–∞\", \"–ë–ª–∏–∑–∫–∞—è –ø–æ–±–µ–¥–∞\", \"–ë–ª–∏–∑–∫–æ–µ –ø–æ—Ä–∞–∂–µ–Ω–∏–µ\", \"–û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–µ –ø–æ—Ä–∞–∂–µ–Ω–∏–µ\"]\n",
    "    source = labels[int(pred[0])]\n",
    "    features_map_stats = features_map_stats.iloc[0].to_dict()\n",
    "    extra_source = features_map_stats\n",
    "    return {\"source\": source, \"extra_source\": extra_source}\n",
    "\n",
    "def predict_match_first(state):\n",
    "    # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å –∏ –ø–æ–ø–∞–ª –≤ —ç—Ç—É –≤–µ—Ç–∫—É\n",
    "    # –¥–ª—è –Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏, –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –Ω–∞–º –Ω—É–∂–Ω—ã: \n",
    "    #   –Ω–∞–∑–≤–∞–Ω–∏—è –¥–≤—É—Ö –∫–æ–º–∞–Ω–¥, \n",
    "    #   –¥–∞—Ç–∞\n",
    "    #       –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, —Ç–æ –±–µ—Ä–µ—Ç—Å—è –ª—é–±–∞—è (–∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–∞ —Å–∞–º—É—é –Ω–µ–¥–∞–≤–Ω—é—é –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º)\n",
    "    #       (–≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –¥–æ–±–∞–≤–∏—Ç—å) –µ—Å–ª–∏ –¥–∞—Ç–∞ —É–∫–∞–∑–∞–Ω–∞, –Ω–æ –º–∞—Ç—á–∞ –Ω–µ—Ç, —Ç–æ –Ω–∞–ø–∏—Å–∞—Ç—å –æ–± —ç—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã —Å –ø–æ–º–æ—â—å—é –ª–ª–º –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ weaviate)\n",
    "    #   –∫–∞—Ä—Ç–∞\n",
    "    #       –µ—Å–ª–∏ –∫–∞—Ä—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, —Ç–æ –±–µ—Ä–µ—Ç—Å—è —Ç–æ–ø3 –∫ –∏ –ø–µ—Ä–µ—á–∏—Å–ª—è–µ—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è (todo++++)\n",
    "    #       –µ—Å–ª–∏ –∫–∞—Ä—Ç–∞ —É–∫–∞–∑–∞–Ω–∞ –∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∞—Ç–∞, –Ω–æ –º–∞—Ç—á–∞ –Ω–µ –Ω–∞—à–ª–æ—Å—å, —Ç–æ –Ω–∞–ø–∏—Å–∞—Ç—å –æ —Ç–æ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –º–∞—Ç—á –Ω–µ –Ω–∞–π–¥–µ–Ω\n",
    "    # –î–ª—è –Ω–∞—á–∞–ª–∞ –ª–ª–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —É–∫–∞–∑–∞–Ω–∞ –ª–∏ –∫–∞—Ä—Ç–∞ –∏ –¥–∞—Ç–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    # –¥–∞–ª–µ–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É –∏ –æ—Ç–¥–∞–µ–º –æ—Ç–≤–µ—Ç\n",
    "    # \n",
    "    #initial_prompt = state[\"initial_prompt\"]\n",
    "    match_predictor_data_check = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [HumanMessage(content=state['initial_prompt'])])\n",
    "    source = json.loads(match_predictor_data_check.content)\n",
    "    return {\"source\": source}\n",
    "        #return \"proceed_match_predict\"\n",
    "    #source = match_metadata_fetch(source)\n",
    "    # —Å–Ω–∞—á–∞–ª–∞ –∑–¥–µ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ (–¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)\n",
    "    # –¥–∞–ª–µ–µ –≤—ã–∑–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏, –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–π –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏\n",
    "    # –¥–∞–ª–µ–µ –ø–µ—Ä–µ–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –≤ –≤–∏–¥–µ initial prompt = question –∏ source = context\n",
    "    print(\"–ø—Ä–æ–∏–∑–æ—à–µ–ª –ø—Ä–µ–¥–∏–∫—Ç –º–∞—Ç—á–∞\")\n",
    "    #initial_prompt = state['initial_prompt']\n",
    "    #source = state[\"source\"]\n",
    "    #loop_step = state.get(\"loop_step, 0\")\n",
    "    #return {\"generation: generation, \"loop_step\": loop_step + 1}\n",
    "    # pretty much the same goes for other analyzers, the only thing is the way it generates using premade prompt\n",
    "\n",
    "def predict_match_second(state):\n",
    "    # –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –º—ã –∑–Ω–∞–µ–º —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–∞–ª –±–æ–ª–µ–µ –º–µ–Ω–µ–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å\n",
    "    initial_prompt = state['initial_prompt']\n",
    "    source = state['source']\n",
    "    extra_source = state['extra_source']\n",
    "    formatted_prompt = predictor_final_prompt.format(\n",
    "        context=source,\n",
    "        user_prompt=initial_prompt,\n",
    "        extra_source=json.dumps(extra_source, ensure_ascii=False))\n",
    "    generation = llm.invoke([HumanMessage(content=formatted_prompt)])\n",
    "    return {\"generation\": generation.content}\n",
    "\n",
    "def rejector(state):\n",
    "    print('–ù–µ –±—ã–ª–∏ —É–∫–∞–∑–∞–Ω—ã –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥ (–∏–ª–∏ –±—ã–ª–æ —É–∫–∞–∑–∞–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã)\\n–ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å, —É–∫–∞–∑–∞–≤ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥')\n",
    "\n",
    "def analyze_shape(state):\n",
    "    print(\"–ø—Ä–æ–∏–∑–æ—à–ª–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ñ–æ—Ä–º—ã –∏–≥—Ä–æ–∫–∞ –∏–ª–∏ –∫–æ–º–∞–Ω–¥—ã\")\n",
    "\n",
    "def analyze_behaviour(state):\n",
    "    print(\"–ø—Ä–æ–∏–∑–æ—à–ª–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –≤–Ω—É—Ç—Ä–∏–∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã\")\n",
    "\n",
    "def fallback(state):\n",
    "    # should finish the generation with proper apology\n",
    "    print('–∏–∑–≤–∏–Ω–∏—Ç–µ—Å—å –∑–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å')\n",
    "\n",
    "def decide_to_generate_predict_match(state):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è —Ä–µ—à–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏\n",
    "    –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Ç–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –¥—Ä—É–≥—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –∏—â–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –¥–∞—Ç–∞—Å—Ç–æ—Ä–µ –∏–ª–∏ –±–¥\n",
    "    (–≤–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏—Ç –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å –Ω–∞ –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å)\n",
    "    –§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±—É–ª–µ–≤–æ –∑–Ω–∞—á–µ–Ω–∏–µ.\n",
    "    \"\"\"\n",
    "    initial_prompt = state[\"initial_prompt\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# edges\n",
    "def route_questions(state):\n",
    "    route_question = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions_ru)]\n",
    "        + [HumanMessage(content=state[\"initial_prompt\"])]\n",
    "    )\n",
    "    source = json.loads(route_question.content)[\"node\"]\n",
    "    if source == \"predictor\":\n",
    "        return \"predict_match\"\n",
    "    elif source == \"shape\":\n",
    "        return \"analyze_shape\"\n",
    "    elif source == \"team\":\n",
    "        return \"analyze_behaviour\"\n",
    "    elif source == \"fallback\":\n",
    "        return \"fallback\"\n",
    "\n",
    "\n",
    "def decide_match_predict_route(state):\n",
    "    source = state[\"source\"]\n",
    "\n",
    "    if source['team_name'] in ['null', 'None'] or source['opponent_name'] in ['null', 'None']:\n",
    "        return \"rejector\"\n",
    "    else:\n",
    "        return \"proceed_match_predict\"\n",
    "\n",
    "\n",
    "\n",
    "hallucination_grader_instructions = ''\n",
    "\n",
    "# do not connect until graders are finished\n",
    "def grade_generation(state):\n",
    "    \"\"\" Decide whether the output is hallucinated or based on stats. Then determine if it is useful or not.\n",
    "    \"\"\"\n",
    "    initial_prompt = state['initial_prompt']\n",
    "    source = state['source']\n",
    "    generation = state['generation']\n",
    "    max_retries = state.get('max_retries', 2) # default to 2 retries\n",
    "\n",
    "    hallucination_grader_instructions_formatted = hallucination_grader_instructions.format(\n",
    "        source = 'smth', generation=generation.content\n",
    "    )\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=hallucination_grader_instructions)]\n",
    "        + [HumanMessage(content=hallucination_grader_instructions_formatted)]\n",
    "    )\n",
    "    grade = json.loads(result.content)[\"binary_score\"]\n",
    "    \n",
    "\n",
    "    if grade == 'yes':\n",
    "        # check here if generation is full enough, useful and answers the question properly\n",
    "        # similar to hallucination grader but with different prompt and output will be binary yes or no again\n",
    "        if grade == 'yes':\n",
    "            return 'useful'\n",
    "        elif state['loop_step'] <= max_retries:\n",
    "            return 'not_useful'\n",
    "        else:\n",
    "            return \"max retries\" #means model couldnt answer the question properly in given max retries\n",
    "    elif state['loop_step'] <= max_retries:\n",
    "        return 'hallucinated' # hallucinated therefore couldnt get useful stats and generated on their own or smth so this will be regeneration attempt\n",
    "    else:\n",
    "        return \"max retries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve_for_match\", retrieve_for_match)  # retriever\n",
    "workflow.add_node(\"predict_match\", predict_match_first)  # predictor\n",
    "workflow.add_node(\"analyze_shape\", analyze_shape)\n",
    "workflow.add_node(\"analyze_behaviour\", analyze_behaviour)\n",
    "workflow.add_node(\"fallback\", fallback)\n",
    "workflow.add_node('rejector', rejector)\n",
    "workflow.add_node('predict_match_second', predict_match_second)\n",
    "\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_questions,\n",
    "    {\n",
    "        \"predict_match\": \"predict_match\",\n",
    "        \"analyze_shape\": \"analyze_shape\",\n",
    "        \"analyze_behaviour\" : \"analyze_behaviour\",\n",
    "        \"fallback\" : \"fallback\"\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"predict_match\",\n",
    "    decide_match_predict_route,\n",
    "    {\n",
    "        \"rejector\": \"rejector\",\n",
    "        \"proceed_match_predict\": \"retrieve_for_match\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"predict_match\", \"retrieve_for_match\")\n",
    "workflow.add_edge('retrieve_for_match', 'predict_match_second')\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "#display(Image(graph.get_graph().draw_mermaid_png())) #not displaying cuz not loading mermaid LOL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_prompt': '–ü–†–∏–≤–µ—Ç —è –∫–æ–∑–∞', 'max_retries': 3, 'loop_step': 0}\n",
      "–∏–∑–≤–∏–Ω–∏—Ç–µ—Å—å –∑–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å\n",
      "{'initial_prompt': '–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—Ç—á–µ heroic –ø—Ä–æ—Ç–∏–≤ Cloud9 –Ω–∞ –∫–∞—Ä—Ç–µ Nuke 24/11/23', 'max_retries': 3, 'loop_step': 0}\n",
      "{'initial_prompt': '–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—Ç—á–µ heroic –ø—Ä–æ—Ç–∏–≤ Cloud9 –Ω–∞ –∫–∞—Ä—Ç–µ Nuke 24/11/23', 'max_retries': 3, 'loop_step': 0, 'source': {'team_name': 'Heroic', 'opponent_name': 'Cloud9', 'date': '24/11/2023', 'map_name': 'Nuke'}}\n",
      "{'initial_prompt': '–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—Ç—á–µ heroic –ø—Ä–æ—Ç–∏–≤ Cloud9 –Ω–∞ –∫–∞—Ä—Ç–µ Nuke 24/11/23', 'max_retries': 3, 'loop_step': 0, 'source': '–û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –ø–æ–±–µ–¥–∞', 'extra_source': {'team_map_winrate': 41.9, 'team_map_pickrate': 14.7, 'team_map_banrate': 62.1, 'opponent_map_winrate': 64.3, 'opponent_map_pickrate': 43.3, 'opponent_map_banrate': 8.6}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643280/14933167.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  source = labels[int(pred[0])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_prompt': '–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—Ç—á–µ heroic –ø—Ä–æ—Ç–∏–≤ Cloud9 –Ω–∞ –∫–∞—Ä—Ç–µ Nuke 24/11/23', 'generation': '–ú–∞—Ç—á –º–µ–∂–¥—É –∫–æ–º–∞–Ω–¥–∞–º–∏ heroic –∏ Cloud9 –Ω–∞ –∫–∞—Ä—Ç–µ Nuke 24 –Ω–æ—è–±—Ä—è 2023 –≥–æ–¥–∞ —Å–ª–æ–∂–∏–ª—Å—è —Ç–∞–∫, –ø–æ—Ç–æ–º—É —á—Ç–æ Cloud9 –∏–º–µ–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –≤–∏–Ω—Ä–µ–π—Ç (64.3%) –∏ —á–∞—Å—Ç–æ—Ç—É –ø–∏–∫–∞ –∫–∞—Ä—Ç—ã (43.3%), —á—Ç–æ –¥–∞–ª–æ –∏–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –≤ –∏–≥—Ä–µ. –≠—Ç–æ, –≤–µ—Ä–æ—è—Ç–Ω–æ, —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞–ª–æ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–π –ø–æ–±–µ–¥–µ Cloud9.', 'max_retries': 3, 'loop_step': 0, 'source': '–û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –ø–æ–±–µ–¥–∞', 'extra_source': {'team_map_winrate': 41.9, 'team_map_pickrate': 14.7, 'team_map_banrate': 62.1, 'opponent_map_winrate': 64.3, 'opponent_map_pickrate': 43.3, 'opponent_map_banrate': 8.6}}\n",
      "{'initial_prompt': '–•–æ—Ä–æ—à–æ –ª–∏ —Å–µ–π—á–∞—Å –∏–≥—Ä–∞–µ—Ç Niko', 'max_retries': 3, 'loop_step': 0}\n",
      "–ø—Ä–æ–∏–∑–æ—à–ª–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ñ–æ—Ä–º—ã –∏–≥—Ä–æ–∫–∞ –∏–ª–∏ –∫–æ–º–∞–Ω–¥—ã\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"initial_prompt\": \"–ü–†–∏–≤–µ—Ç —è –∫–æ–∑–∞\", \"max_retries\": 3}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "#inputs = {\"initial_prompt\": \"–†–∞—Å—Å–∫–∞–∂–∏ –æ –∑–∞–≤—Ç—Ä–∞—à–Ω–µ–º –º–∞—Ç—á–µ Navi vs Falcons\", \"max_retries\": 3}\n",
    "#for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "#    print(event)\n",
    "\n",
    "inputs = {\"initial_prompt\": \"–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—Ç—á–µ heroic –ø—Ä–æ—Ç–∏–≤ Cloud9 –Ω–∞ –∫–∞—Ä—Ç–µ Nuke 24/11/23\", \"max_retries\": 3}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event) \n",
    "inputs = {\"initial_prompt\": \"–•–æ—Ä–æ—à–æ –ª–∏ —Å–µ–π—á–∞—Å –∏–≥—Ä–∞–µ—Ç Niko\", \"max_retries\": 3}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
