{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AlexxFlexing/RAG/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    #model_name = \"ai-forever/sbert_large_mt_nlu_ru\",\n",
    "    model_name = \"sergeyzh/rubert-mini-uncased\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True} # i wanted to explicit set this to False since we are using weaviate, but DONT FORGET TO SET IT TO TRUE if we stop using weaviate. SET IT TO TRUE if vector db doesnt normalize automatically\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/AlexxFlexing/RAG/venv/lib/python3.12/site-packages/weaviate/warnings.py:340: UserWarning: Con006: You're using the sync client in an async context. This usage is discouraged to avoid blocking your async event loop with sync I/O calls.\n",
      "            We encourage you to update your code to use the async client instead when running inside async def functions!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x73e94f586c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader #not sure about those 2\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader #i mean this as second\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "#series_result = '../data/processed/series_results.csv'\n",
    "#df = pd.read_csv(series_result)\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    host=\"127.0.0.1\",  # Use a string to specify the host\n",
    "    port=8080,\n",
    "    grpc_port=50051,\n",
    ")\n",
    "\n",
    "if client.collections.exists(\"MatchData\"): #redo this in prod obviously\n",
    "    client.collections.delete(\"MatchData\")\n",
    "\n",
    "if client.collections.exists(\"MapStats\"):\n",
    "    client.collections.delete(\"MapStats\")\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"MatchData\",\n",
    "    properties=[\n",
    "        Property(name=\"team_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"team_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"date\", data_type=DataType.TEXT),\n",
    "        Property(name=\"opponent_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"opponent_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"map_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"series_result\", data_type=DataType.TEXT),\n",
    "        Property(name=\"description\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    description=\"Информация о матче между командами\",\n",
    "    vectorizer_config=Configure.Vectorizer.none()  # for manual embedding\n",
    ")\n",
    "\n",
    "# Create MapStats class\n",
    "client.collections.create(\n",
    "    name=\"MapStats\",\n",
    "    properties=[\n",
    "        Property(name=\"team_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"map_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"winrate\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"pickrate\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"banrate\", data_type=DataType.NUMBER),\n",
    "    ],\n",
    "    description=\"Статистика по картам для команд\",\n",
    "    vectorizer_config=Configure.Vectorizer.none()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 456/456 [00:02<00:00, 169.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"sergeyzh/rubert-mini-uncased\")\n",
    "\n",
    "series_result = '../data/processed/series_results.csv'\n",
    "maps_stats = '../data/processed/team_maps.csv'\n",
    "\n",
    "matches = pd.read_csv(series_result)\n",
    "matches.drop(columns=['series_link', 'series_result'])\n",
    "maps = pd.read_csv(maps_stats)\n",
    "\n",
    "\n",
    "\n",
    "matches[\"description\"] = matches.apply(\n",
    "    lambda row: f\"команда {row['team_name']} сыграла против команды {row['opponent_name']} на карте {row['map_name']}, матч был проведен {row['date']}\",\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = model.encode(matches[\"description\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "collection_match = client.collections.get(\"MatchData\") #fix collections later maybe\n",
    "\n",
    "def json_clean(obj):\n",
    "    for k, v in obj.items():\n",
    "        if isinstance(v, float) and (np.isnan(v) or np.isinf(v)):\n",
    "            obj[k] = 0.0\n",
    "    return obj\n",
    "\n",
    "\n",
    "for i, row in matches.iterrows():\n",
    "    properties = {\n",
    "        \"team_id\": str(row[\"team_id\"]),\n",
    "        \"team_name\": row[\"team_name\"],\n",
    "        \"date\": row[\"date\"],\n",
    "        \"opponent_id\": str(row[\"opponent_id\"]),\n",
    "        \"opponent_name\": row[\"opponent_name\"],\n",
    "        \"map_name\": row[\"map_name\"],\n",
    "        \"series_result\": str(row[\"series_result\"]),\n",
    "        \"description\": row[\"description\"],\n",
    "    }\n",
    "    properties = json_clean(properties) # had to\n",
    "    vector = embeddings[i]\n",
    "    collection_match.data.insert(properties=properties, vector=vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "collection_team_map = client.collections.get(\"MapStats\")\n",
    "df = pd.read_csv(maps_stats)\n",
    "\n",
    "def extract_float(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    match = re.search(r\"[\\d.]+\", str(value))\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "for col in ['winrate', 'pickrate', 'banrate']:\n",
    "    df[col] = df[col].apply(extract_float)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    properties = {\n",
    "        \"team_id\": str(row[\"team_id\"]),\n",
    "        \"map_name\": row[\"map_name\"],\n",
    "        \"winrate\": float(row[\"winrate\"]),\n",
    "        \"pickrate\": float(row[\"pickrate\"]),\n",
    "        \"banrate\": float(row[\"banrate\"]),\n",
    "    }\n",
    "    collection_team_map.data.insert(properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "match_vectorstore = WeaviateVectorStore(\n",
    "    client=client,\n",
    "    index_name=\"MatchData\",  # название класса\n",
    "    text_key=\"description\",  # по какому полю искать\n",
    "    embedding=embedding_model,    # эмбеддинги для векторного сравнения\n",
    ")\n",
    "\n",
    "match_retriever_certain = match_vectorstore.as_retriever(search_kwargs={\"k\":1}) # может быть не один, а даже 5, если bo5. думать.\n",
    "match_retriever_bo3 = match_vectorstore.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_instructions_ru = \"\"\"Ты эксперт по классификации запроса пользователя. Твоя задача — направить пользователя к одному из агентов.\n",
    "\n",
    "Агенты:\n",
    "- предсказатель результатов матча → \"predictor\"\n",
    "- аналитик формы игрока или команды → \"shape\"\n",
    "- анализ игрового поведения команды → \"team\"\n",
    "- свободные диалоги → \"fallback\"\n",
    "\n",
    "В качестве ответа верни **ТОЛЬКО** JSON-объект с ключом \"node\", одно из значений: \"predictor\", \"shape\", \"team\", \"fallback\".\n",
    "\n",
    "**Пример:**\n",
    "{\"node\": \"predictor\"}\n",
    "\n",
    "Не добавляй никаких объяснений. Не веди диалог. Только JSON.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# возможно стоит добавить bias в сторону fallback, чтобы модель чаще выдавала его для уточнения запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"ilyagusev/saiga_llama3:latest\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 'predictor'} {'node': 'fallback'} {'node': 'shape'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "\n",
    "test_case_match = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions_ru)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"кто выиграет в завтрашнем матче: Navi или G2?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "test_case_fallback = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions_ru)]\n",
    "    + [HumanMessage(content=\"Я вкусно покушал\")]\n",
    ")\n",
    "test_case_shape = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions_ru)]\n",
    "    + [HumanMessage(content=\"В какой форме находится Niko?\")]\n",
    ")\n",
    "print(\n",
    "    json.loads(test_case_match.content),\n",
    "    json.loads(test_case_fallback.content),\n",
    "    json.loads(test_case_shape.content),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Close win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643280/374847762.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Prediction:\", labels[int(pred[0])])\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "closeness_analyzer_model = CatBoostClassifier()\n",
    "closeness_analyzer_model.load_model(\"catboost_series_model.cbm\")\n",
    "\n",
    "sample_feature = pd.DataFrame([[55.0, 10.0, 20.0, 50.0, 8.0, 30.0]], columns=[\n",
    "    'team_map_winrate', 'team_map_pickrate', 'team_map_banrate',\n",
    "    'opponent_map_winrate', 'opponent_map_pickrate', 'opponent_map_banrate'\n",
    "])\n",
    "\n",
    "# Predict\n",
    "pred = closeness_analyzer_model.predict(sample_feature)\n",
    "\n",
    "labels = [\"One-sided win\", \"Close win\", \"Close loss\", \"One-sided loss\"]\n",
    "print(\"Prediction:\", labels[int(pred[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team_name': 'Natus Vincere', 'opponent_name': 'G2 Esports', 'date': None, 'map_name': 'Mirage'} {'team_name': 'Team Spirit', 'opponent_name': 'Virtus Pro', 'date': '25/10/2023', 'map_name': None} {'team_name': None, 'opponent_name': None, 'date': 'null', 'map_name': None}\n",
      "{'team_name': 'null', 'opponent_name': 'null'}\n"
     ]
    }
   ],
   "source": [
    "# match predictor section\n",
    "test_case_prompt = \"\"\"Твоя задача - извлечь названия команд из запроса пользователя, если они указаны. Если названия команды нет, то укажи 'null'\n",
    "Ответ должен быть возвращен в формате JSON со следующими ключами:\n",
    "{\n",
    "    \"team_name\": \"<название первой команды или null>\",\n",
    "    \"opponent_name\": \"<название второй команды или null>\"    \n",
    "}\n",
    "Не добавляй никаких комментариев, возвращай чистый JSON-объект.\n",
    "\"\"\"\n",
    "match_predictor_prompt = \"\"\"Ты — интеллектуальный парсер пользовательских запросов. Твоя задача — извлечь из текста следующие данные:\n",
    "В твоей памяти нет абсолютно никаких игр.\n",
    "🔹 Обязательные данные:\n",
    "- Название первой команды (team_name)\n",
    "- Название второй команды (opponent_name)\n",
    "\n",
    "🔹 Дополнительные данные (если указаны):\n",
    "- Дата матча (date) — обязательно в формате dd/mm/yyyy (например: 25/11/2024)\n",
    "- Название карты (map_name)\n",
    "\n",
    "Ответ должен быть возвращён строго в формате JSON со следующими ключами:\n",
    "\n",
    "{\n",
    "  \"team_name\": \"<название первой команды или null>\",\n",
    "  \"opponent_name\": \"<название второй команды или null>\",\n",
    "  \"date\": \"<дата в формате dd/mm/yyyy или null>\",\n",
    "  \"map_name\": \"<название карты или null>\"\n",
    "}\n",
    "\n",
    "Если какие-либо из данных отсутствуют в пользовательском запросе, укажи значение `null` (это JSON null, без кавычек). Не добавляй никаких комментариев или текста вне JSON — только чистый JSON-объект.\n",
    "Если названия команды нет, то укажи 'null'\n",
    "Если названия команды нет, то укажи 'null'\n",
    "Если названия команды нет, то укажи 'null'\n",
    "Пример корректного ответа:\n",
    "\n",
    "{\n",
    "  \"team_name\": \"Team Spirit\",\n",
    "  \"opponent_name\": \"Natus Vincere\",\n",
    "  \"date\": \"25/11/2024\",\n",
    "  \"map_name\": \"Mirage\"\n",
    "}\n",
    "\n",
    "Будь внимателен к формулировкам — названия команд, дату и карту могут указывать в свободной форме. Твоя задача — точно извлечь данные и вернуть их в необходимом формате.\n",
    "\"\"\"\n",
    "\n",
    "test_case_match_pred_1 = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"насколько близкой была игра Navi - G2 на карте мираж\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "test_case_match_pred_2 = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [HumanMessage(content=\"проанализируй игру Team Spirit против Virtus Pro 25 октября 2023 года\")]\n",
    ")\n",
    "test_case_match_pred_3 = llm_json_mode.invoke(       # fix this ffs\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [HumanMessage(content=\"Проанализируй вчерашнюю игру\")]\n",
    ")\n",
    "print(\n",
    "    json.loads(test_case_match_pred_1.content),\n",
    "    json.loads(test_case_match_pred_2.content),\n",
    "    json.loads(test_case_match_pred_3.content),\n",
    ")\n",
    "\n",
    "hard_test_case = llm_json_mode.invoke([SystemMessage(content=test_case_prompt)]\n",
    "    + [HumanMessage(content=\"Проанализируй вчерашнюю игру\")]\n",
    ")\n",
    "\n",
    "print(json.loads(hard_test_case.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    initial_prompt: str # initial user prompt\n",
    "    generation: str # LLM generation\n",
    "    max_retries: int # max number of retries for answering\n",
    "    answers: int # number of answers generated\n",
    "    loop_step: Annotated[int, operator.add] # have to use annotated int since using default int will lead into multiple edges not being able to combine values properly (and some other stuff)\n",
    "    source: List[str] # stats or any other retrieved valuables\n",
    "    #source: dict\n",
    "    #extra_source: dict\n",
    "    extra_source: List[str] #i ve made this source just to be sure it is not overridden, del later or fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Close win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643280/85189475.py:50: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Prediction:\", labels[int(pred[0])])\n"
     ]
    }
   ],
   "source": [
    "match_doc = match_retriever_certain.invoke('Heroic против G2')\n",
    "def match_metadata_fetch(match_doc):\n",
    "    matchdoc_data = match_doc[0].metadata\n",
    "    team_id = str(re.sub(r'\\.\\d*', '', matchdoc_data['team_id']))\n",
    "    opponent_id = re.sub(r'\\.\\d*', '', matchdoc_data['opponent_id'])\n",
    "    map_name = matchdoc_data['map_name'] #dont ask why those str turned into float\n",
    "    return team_id, opponent_id, map_name\n",
    "\n",
    "team_id, opponent_id, map_name = match_metadata_fetch(match_doc)\n",
    "\n",
    "\n",
    "def team_map_stats(team_id, opponent_id, map_name):\n",
    "    response = collection_team_map.query.fetch_objects(\n",
    "        filters=(Filter.by_property(\"team_id\").equal(team_id)) & Filter.by_property(\"map_name\").equal(map_name)\n",
    "    )\n",
    "    if response.objects:\n",
    "        team1_winrate = response.objects[0].properties['winrate']\n",
    "        team1_pickrate = response.objects[0].properties['pickrate']\n",
    "        team1_banrate = response.objects[0].properties['banrate']\n",
    "\n",
    "    response = collection_team_map.query.fetch_objects(\n",
    "        filters=(Filter.by_property(\"team_id\").equal(opponent_id)) & Filter.by_property(\"map_name\").equal(map_name)\n",
    "    )\n",
    "    if response.objects:\n",
    "        team2_winrate = response.objects[0].properties['winrate']\n",
    "        team2_pickrate = response.objects[0].properties['pickrate']\n",
    "        team2_banrate = response.objects[0].properties['banrate']\n",
    "    df = pd.DataFrame([[\n",
    "        team1_winrate,\n",
    "        team1_pickrate,\n",
    "        team1_banrate,\n",
    "        team2_winrate,\n",
    "        team2_pickrate,\n",
    "        team2_banrate\n",
    "    ]], columns=[\n",
    "        'team_map_winrate',\n",
    "        'team_map_pickrate',\n",
    "        'team_map_banrate',\n",
    "        'opponent_map_winrate',\n",
    "        'opponent_map_pickrate',\n",
    "        'opponent_map_banrate'\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "sample_map_stats = team_map_stats(team_id, opponent_id, map_name)\n",
    "\n",
    "pred = closeness_analyzer_model.predict(sample_map_stats)\n",
    "labels = [\"One-sided win\", \"Close win\", \"Close loss\", \"One-sided loss\"]\n",
    "print(\"Prediction:\", labels[int(pred[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_source = {\\n    \"date\":\\'null\\',\\n    \"map_name\":\\'Mirage\\'\\n}\\n\\nsample_query = \\'Heroic против G2 на Mirage\\'\\n\\nfilter_sample = []\\nif sample_source[\\'date\\'] != \\'null\\' or sample_source[\\'date\\'] != \\'None\\':\\n    filter_sample.append(Filter.by_property(\"date\").equal(sample_source[\"date\"]))\\nif sample_source[\\'map_name\\'] != \\'null\\' or sample_source[\\'map_name\\'] != \\'None\\':\\n    filter_sample.append(Filter.by_property(\"map_name\").equal(sample_source[\"map_name\"]))\\n\\ncombined_filter = None\\nif filter_sample:\\n    combined_filter = Filter.all_of([*filter_sample])\\nresponse = collection_match.query.near_text(\\n    query=sample_query,\\n    filters=combined_filter,\\n    limit=3\\n)\\n\\nprint(response)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is query + filter\n",
    "# it is not working\n",
    "# why?\n",
    "# because i fucked up big time\n",
    "# how?\n",
    "# by tweaking manual embedding\n",
    "\n",
    "\"\"\"sample_source = {\n",
    "    \"date\":'null',\n",
    "    \"map_name\":'Mirage'\n",
    "}\n",
    "\n",
    "sample_query = 'Heroic против G2 на Mirage'\n",
    "\n",
    "filter_sample = []\n",
    "if sample_source['date'] != 'null' or sample_source['date'] != 'None':\n",
    "    filter_sample.append(Filter.by_property(\"date\").equal(sample_source[\"date\"]))\n",
    "if sample_source['map_name'] != 'null' or sample_source['map_name'] != 'None':\n",
    "    filter_sample.append(Filter.by_property(\"map_name\").equal(sample_source[\"map_name\"]))\n",
    "\n",
    "combined_filter = None\n",
    "if filter_sample:\n",
    "    combined_filter = Filter.all_of([*filter_sample])\n",
    "response = collection_match.query.near_text(\n",
    "    query=sample_query,\n",
    "    filters=combined_filter,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "print(response)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# очень глупый и простой промпт \n",
    "predictor_final_prompt = \"\"\"Ты — эксперт по анализу матчей в киберспорте, работающий над задачами вопрос-ответ в рамках интеллектуального агента.\n",
    "\n",
    "Вот достоверный контекст, который необходимо использовать при формулировании ответа:\n",
    "\n",
    "{context}\n",
    "\n",
    "Ниже представлен пользовательский запрос, описывающий интересующий матч:\n",
    "\n",
    "{user_prompt}\n",
    "\n",
    "Также предоставлены дополнительные статистические данные по матчу:\n",
    "\n",
    "{extra_source}\n",
    "\n",
    "Дополнительные данные содержат числовую статистику по карте в следующем порядке:\n",
    "- team_map_winrate — винрейт команды на карте\n",
    "- team_map_pickrate — частота пика карты командой\n",
    "- team_map_banrate — частота бана карты командой\n",
    "- opponent_map_winrate — винрейт соперника на карте\n",
    "- opponent_map_pickrate — частота пика карты соперником\n",
    "- opponent_map_banrate — частота бана карты соперником\n",
    "\n",
    "Твоя задача:\n",
    "1. Принять контекст как полностью достоверную информацию.\n",
    "2. Использовать статистику из дополнительного источника, чтобы объяснить, почему матч сложился именно так.\n",
    "3. Учитывать содержание пользовательского запроса при формировании объяснения.\n",
    "4. Не придумывать информацию и не использовать внешние источники — опирайся только на данные, представленные выше.\n",
    "\n",
    "Дай чёткий и логичный ответ на русском языке. Максимум 3 предложения. Ответ должен быть кратким и по существу.\n",
    "\n",
    "Ответ:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document #for docs but remove before pushing\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# nodes\n",
    "def retrieve_for_match(state):\n",
    "    # надо найти матч, используя фильтры\n",
    "    # но фильтры использовать не удалось, потому что я использовал мануал эмбеддинг ранее (хаахахахахахах)\n",
    "    # и теперь у меня проблема, в одной из закоменченных cells выше о ней написано\n",
    "    # поэтому мы тупо находим по вектору дескрипшна\n",
    "    # забивая, к сожалению, на фильтры\n",
    "    source_raw = match_retriever_certain.invoke(state['initial_prompt'])\n",
    "    # нашли матч и далее работаем с этим\n",
    "    matchdoc_data = source_raw[0].metadata # и тут мы берем всего лишь первый объект, хотя мы его берем еще с помощью top k, где k=1 используя retriever_certain\n",
    "    team_id = str(re.sub(r'\\.\\d*', '', matchdoc_data['team_id']))\n",
    "    opponent_id = re.sub(r'\\.\\d*', '', matchdoc_data['opponent_id'])\n",
    "    map_name = matchdoc_data['map_name'] #dont ask why those str turned into float\n",
    "    features_map_stats = team_map_stats(team_id, opponent_id, map_name) # по хорошему нужно рефакторить код, чтобы та функция была где нибудь здесь)\n",
    "    pred = closeness_analyzer_model.predict(features_map_stats)\n",
    "    labels = [\"Односторонняя победа\", \"Близкая победа\", \"Близкое поражение\", \"Одностороннее поражение\"]\n",
    "    source = labels[int(pred[0])]\n",
    "    features_map_stats = features_map_stats.iloc[0].to_dict()\n",
    "    extra_source = features_map_stats\n",
    "    return {\"source\": source, \"extra_source\": extra_source}\n",
    "\n",
    "def predict_match_first(state):\n",
    "    # пользователь задал вопрос и попал в эту ветку\n",
    "    # для начала необходимо проверить полноту данных для проведения аналитики, в нашем случае нам нужны: \n",
    "    #   названия двух команд, \n",
    "    #   дата\n",
    "    #       если не указана, то берется любая (исправить на самую недавнюю в дальнейшем)\n",
    "    #       (в дальнейшем добавить) если дата указана, но матча нет, то написать об этом пользователю (извлечение и преобразование даты с помощью ллм и дальнейшее использование фильтра weaviate)\n",
    "    #   карта\n",
    "    #       если карта не указана, то берется топ3 к и перечисляется пользователю для уточнения (todo++++)\n",
    "    #       если карта указана и указана дата, но матча не нашлось, то написать о том, что данный матч не найден\n",
    "    # Для начала ллм определит указана ли карта и дата в запросе пользователя\n",
    "    # далее проверяем наличие данных по запросу и отдаем ответ\n",
    "    # \n",
    "    #initial_prompt = state[\"initial_prompt\"]\n",
    "    match_predictor_data_check = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=match_predictor_prompt)]\n",
    "    + [HumanMessage(content=state['initial_prompt'])])\n",
    "    source = json.loads(match_predictor_data_check.content)\n",
    "    return {\"source\": source}\n",
    "        #return \"proceed_match_predict\"\n",
    "    #source = match_metadata_fetch(source)\n",
    "    # сначала здесь проверить полноту данных для проведения аналитики (данные, которые предоставил пользователь)\n",
    "    # далее вызвать функцию инференса модели, написанной для аналитики\n",
    "    # далее передать вопрос пользователя и ответ модели в виде initial prompt = question и source = context\n",
    "    print(\"произошел предикт матча\")\n",
    "    #initial_prompt = state['initial_prompt']\n",
    "    #source = state[\"source\"]\n",
    "    #loop_step = state.get(\"loop_step, 0\")\n",
    "    #return {\"generation: generation, \"loop_step\": loop_step + 1}\n",
    "    # pretty much the same goes for other analyzers, the only thing is the way it generates using premade prompt\n",
    "\n",
    "def predict_match_second(state):\n",
    "    # на этом этапе мы знаем что пользователь дал более менее корректный запрос\n",
    "    initial_prompt = state['initial_prompt']\n",
    "    source = state['source']\n",
    "    extra_source = state['extra_source']\n",
    "    formatted_prompt = predictor_final_prompt.format(\n",
    "        context=source,\n",
    "        user_prompt=initial_prompt,\n",
    "        extra_source=json.dumps(extra_source, ensure_ascii=False))\n",
    "    generation = llm.invoke([HumanMessage(content=formatted_prompt)])\n",
    "    return {\"generation\": generation.content}\n",
    "\n",
    "def rejector(state):\n",
    "    print('Не были указаны названия команд (или было указано название только для одной команды)\\nПовторите ваш запрос, указав названия команд')\n",
    "\n",
    "def analyze_shape(state):\n",
    "    print(\"произошла аналитика формы игрока или команды\")\n",
    "\n",
    "def analyze_behaviour(state):\n",
    "    print(\"произошла аналитика внутриигрового поведения команды\")\n",
    "\n",
    "def fallback(state):\n",
    "    # should finish the generation with proper apology\n",
    "    print('извинитесь за ваш запрос')\n",
    "\n",
    "def decide_to_generate_predict_match(state):\n",
    "    \"\"\"\n",
    "    Функция решает достаточно ли данных пользователь предоставил для проведения аналитики\n",
    "    Если данных недостаточно, то ссылается на другую функцию, которая ищет данные в датасторе или бд\n",
    "    (возможно стоит доработать, чтобы данные проверялись на правдивость)\n",
    "    Функция возвращает булево значение.\n",
    "    \"\"\"\n",
    "    initial_prompt = state[\"initial_prompt\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# edges\n",
    "def route_questions(state):\n",
    "    route_question = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions_ru)]\n",
    "        + [HumanMessage(content=state[\"initial_prompt\"])]\n",
    "    )\n",
    "    source = json.loads(route_question.content)[\"node\"]\n",
    "    if source == \"predictor\":\n",
    "        return \"predict_match\"\n",
    "    elif source == \"shape\":\n",
    "        return \"analyze_shape\"\n",
    "    elif source == \"team\":\n",
    "        return \"analyze_behaviour\"\n",
    "    elif source == \"fallback\":\n",
    "        return \"fallback\"\n",
    "\n",
    "\n",
    "def decide_match_predict_route(state):\n",
    "    source = state[\"source\"]\n",
    "\n",
    "    if source['team_name'] in ['null', 'None'] or source['opponent_name'] in ['null', 'None']:\n",
    "        return \"rejector\"\n",
    "    else:\n",
    "        return \"proceed_match_predict\"\n",
    "\n",
    "\n",
    "\n",
    "hallucination_grader_instructions = ''\n",
    "\n",
    "# do not connect until graders are finished\n",
    "def grade_generation(state):\n",
    "    \"\"\" Decide whether the output is hallucinated or based on stats. Then determine if it is useful or not.\n",
    "    \"\"\"\n",
    "    initial_prompt = state['initial_prompt']\n",
    "    source = state['source']\n",
    "    generation = state['generation']\n",
    "    max_retries = state.get('max_retries', 2) # default to 2 retries\n",
    "\n",
    "    hallucination_grader_instructions_formatted = hallucination_grader_instructions.format(\n",
    "        source = 'smth', generation=generation.content\n",
    "    )\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=hallucination_grader_instructions)]\n",
    "        + [HumanMessage(content=hallucination_grader_instructions_formatted)]\n",
    "    )\n",
    "    grade = json.loads(result.content)[\"binary_score\"]\n",
    "    \n",
    "\n",
    "    if grade == 'yes':\n",
    "        # check here if generation is full enough, useful and answers the question properly\n",
    "        # similar to hallucination grader but with different prompt and output will be binary yes or no again\n",
    "        if grade == 'yes':\n",
    "            return 'useful'\n",
    "        elif state['loop_step'] <= max_retries:\n",
    "            return 'not_useful'\n",
    "        else:\n",
    "            return \"max retries\" #means model couldnt answer the question properly in given max retries\n",
    "    elif state['loop_step'] <= max_retries:\n",
    "        return 'hallucinated' # hallucinated therefore couldnt get useful stats and generated on their own or smth so this will be regeneration attempt\n",
    "    else:\n",
    "        return \"max retries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve_for_match\", retrieve_for_match)  # retriever\n",
    "workflow.add_node(\"predict_match\", predict_match_first)  # predictor\n",
    "workflow.add_node(\"analyze_shape\", analyze_shape)\n",
    "workflow.add_node(\"analyze_behaviour\", analyze_behaviour)\n",
    "workflow.add_node(\"fallback\", fallback)\n",
    "workflow.add_node('rejector', rejector)\n",
    "workflow.add_node('predict_match_second', predict_match_second)\n",
    "\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_questions,\n",
    "    {\n",
    "        \"predict_match\": \"predict_match\",\n",
    "        \"analyze_shape\": \"analyze_shape\",\n",
    "        \"analyze_behaviour\" : \"analyze_behaviour\",\n",
    "        \"fallback\" : \"fallback\"\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"predict_match\",\n",
    "    decide_match_predict_route,\n",
    "    {\n",
    "        \"rejector\": \"rejector\",\n",
    "        \"proceed_match_predict\": \"retrieve_for_match\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"predict_match\", \"retrieve_for_match\")\n",
    "workflow.add_edge('retrieve_for_match', 'predict_match_second')\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "#display(Image(graph.get_graph().draw_mermaid_png())) #not displaying cuz not loading mermaid LOL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_prompt': 'ПРивет я коза', 'max_retries': 3, 'loop_step': 0}\n",
      "извинитесь за ваш запрос\n",
      "{'initial_prompt': 'Расскажи о матче heroic против Cloud9 на карте Nuke 24/11/23', 'max_retries': 3, 'loop_step': 0}\n",
      "{'initial_prompt': 'Расскажи о матче heroic против Cloud9 на карте Nuke 24/11/23', 'max_retries': 3, 'loop_step': 0, 'source': {'team_name': 'Heroic', 'opponent_name': 'Cloud9', 'date': '24/11/2023', 'map_name': 'Nuke'}}\n",
      "{'initial_prompt': 'Расскажи о матче heroic против Cloud9 на карте Nuke 24/11/23', 'max_retries': 3, 'loop_step': 0, 'source': 'Односторонняя победа', 'extra_source': {'team_map_winrate': 41.9, 'team_map_pickrate': 14.7, 'team_map_banrate': 62.1, 'opponent_map_winrate': 64.3, 'opponent_map_pickrate': 43.3, 'opponent_map_banrate': 8.6}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643280/14933167.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  source = labels[int(pred[0])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_prompt': 'Расскажи о матче heroic против Cloud9 на карте Nuke 24/11/23', 'generation': 'Матч между командами heroic и Cloud9 на карте Nuke 24 ноября 2023 года сложился так, потому что Cloud9 имели более высокий винрейт (64.3%) и частоту пика карты (43.3%), что дало им преимущество в игре. Это, вероятно, способствовало односторонней победе Cloud9.', 'max_retries': 3, 'loop_step': 0, 'source': 'Односторонняя победа', 'extra_source': {'team_map_winrate': 41.9, 'team_map_pickrate': 14.7, 'team_map_banrate': 62.1, 'opponent_map_winrate': 64.3, 'opponent_map_pickrate': 43.3, 'opponent_map_banrate': 8.6}}\n",
      "{'initial_prompt': 'Хорошо ли сейчас играет Niko', 'max_retries': 3, 'loop_step': 0}\n",
      "произошла аналитика формы игрока или команды\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"initial_prompt\": \"ПРивет я коза\", \"max_retries\": 3}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "#inputs = {\"initial_prompt\": \"Расскажи о завтрашнем матче Navi vs Falcons\", \"max_retries\": 3}\n",
    "#for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "#    print(event)\n",
    "\n",
    "inputs = {\"initial_prompt\": \"Расскажи о матче heroic против Cloud9 на карте Nuke 24/11/23\", \"max_retries\": 3}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event) \n",
    "inputs = {\"initial_prompt\": \"Хорошо ли сейчас играет Niko\", \"max_retries\": 3}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
